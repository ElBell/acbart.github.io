---
layout: post
title: "You Didn't Know about X, So I Don't Care"
author: Austin Cory Bart
tags: [logical fallacies, computer science education research]
---

I don't know what logical fallacy this is, but it comes up a fair bit in our field when it comes to peer review. Someone writes a paper or creates a tool. They fail to cite a particular paper. Maybe that paper is related, maybe it is fundamental, maybe it exactly duplicates the work. Either way, someone sees that they didn't refer to that particular paper, and they reject the whole body of work based on that.

I'm sure most people have encountered this when having papers rejected. It's not so bad on a journal rejection, because there's an opportunity to fix things up. It's more frustrating with SIGCSE/ITiCSE/ICER, since you don't get a chance to respond. But it happens in person too. On several occasions, I've seen more senior researchers shut down younger ones. "X has been doing that since the 80s!"  

I've done this myself. At a SIGCSE several years back, I came across an autograding platform for some institution, as a poster. I asked them how it compared to Web-CAT, and they didn't know what I was referring to. What?! How could they not know about Web-CAT! Everyone knows about Web-CAT! I mentioned they should look into it, and then moved on. Surely there was nothing to see here, if they didn't even know about Web-CAT.

It's unreasonable to expect people to know about everything. In particular, searching the ACM Digital Library is not very good. I can know exactly what paper I want, and still struggle to find it. Google Scholar tends to give me better results, but it doesn't cover everything. I'm pleased with new systems like [csedresearch](http://csedresearch.org/), which actually lets me filter and search papers, but it's still not a complete representation of CS Ed tools and research (seems to be getting better every day, though).

So to myself, and to others, I hope we all cut the new kids some slack. It's hard to be aware of all the different tools out there. Did someone make yet another autograder? Great, let's see what innovations they brought to the table, and gently help them understand what's been done. Did we create another introductory curriculum? Cool, let's see what we can learn from using it and how it compares to others' work. Don't just shut them down because "X has been doing that since the 80s!" - tools change, contexts change, people change, and I think it's okay to have a little temporal redundancy.